# ğŸ§  Mood Timeline â€“ Segment-Level Video Emotion Analyzer

---

## ğŸ¯ Overview  

**Mood Timeline** is a prototype feature built as part of my **College Minor Project**, designed to analyze any video and detect its *emotional tone (Calm / Neutral / Hype)* across different segments.  

It visually represents these emotions as a **color-coded timeline** under the video player â€” helping users quickly jump to their preferred mood-based parts of a video.  

This project is part of a larger idea â€” an **AI-powered Video Summarization Extension** that can later work on platforms like **YouTube, Coursera, and Udemy**.  

> ğŸ§© A **sample video and transcript** are included in this repository only for demonstration purposes.

---

## âš™ï¸ Features  

- ğŸ¬ **Video Segmentation** â€“ Splits video into multiple segments using time and transcript boundaries.  
- ğŸ˜„ **Mood Detection** â€“ Detects emotion (Calm / Neutral / Hype) per segment using transcript, prosody (audio), and visuals.  
- ğŸ¨ **Mood Timeline UI** â€“ Displays color-coded mood changes below the video scrubber.  
  - ğŸŸ¦ Calm  
  - ğŸŸ¨ Neutral  
  - ğŸŸ¥ Hype  
- ğŸšï¸ **Mood Filter** â€“ Filter or jump to parts of the video based on mood.  
- ğŸŒ **API Endpoint** â€“ Provides mood predictions with confidence scores for each segment.  

---

## ğŸ§© Tech Stack  

| Layer | Technology |
|-------|-------------|
| **Frontend** | React (Vite) + Tailwind CSS |
| **Backend** | FastAPI (Python) |
| **Speech Analysis** | Librosa / PyAudioAnalysis |
| **Transcript Extraction** | YouTube Transcript API |
| **Text Sentiment Model** | Hugging Face Transformers (DistilBERT / VADER) |
| **Visualization** | Custom React component (MoodTimeline.jsx) |

---

## ğŸ—‚ï¸ Project Structure  

```bash
mood-timeline-prototype/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # Main FastAPI server
â”‚   â”œâ”€â”€ utils.py               # Mood prediction & segmentation logic
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ package.json           # React app configuration
â”‚   â”œâ”€â”€ vite.config.js         # Vite setup
â”‚   â”œâ”€â”€ index.html             # App entry point
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx            # Main React component
â”‚       â””â”€â”€ components/
â”‚           â””â”€â”€ MoodTimeline.jsx   # Mood timeline UI component
â”‚
â”œâ”€â”€ sample_transcript.txt      # Sample transcript for demo
â”œâ”€â”€ sample_video.mp4           # Sample video for reference
â””â”€â”€ README.md                  # Documentation
```
---

## ğŸš€ How It Works

1. **Input Collection**  
   User provides either a local video file or a YouTube URL.  
   - If YouTube URL â†’ system uses **`youtube-transcript-api`** to fetch auto-generated subtitles.  

2. **Segmentation**  
   Transcript is divided into short sentences or scene-level segments (usually 20â€“30 seconds).  

3. **Feature Extraction**  
   Each segment is analyzed using three data types:  
   - **Text (Lexical):** Extracts sentiment polarity from transcript words.  
   - **Audio (Prosodic):** Measures pitch, energy, and volume variance using libraries like `librosa`.  
   - **Visual (Frames):** Captures brightness and color intensity from thumbnails every few seconds.  

4. **Mood Classification**  
   Based on the extracted features, the system classifies each segment into:  
   - ğŸŸ¦ Calm â†’ Low energy, neutral tone  
   - ğŸŸ¨ Neutral â†’ Balanced speech and words  
   - ğŸŸ¥ Hype â†’ High excitement and intensity  

5. **Visualization**  
   - The React frontend renders a **color-coded timeline** below the video scrubber.  
   - Hovering shows timestamp, mood label, and confidence score.  
   - A dropdown lets users filter and jump directly to specific moods.
---

## ğŸ§  API Endpoints

ğŸ§ 1. **Upload Video**
```bash
POST /upload
Body: multipart/form-data { file: video.mp4 }
Response: { videoId: "abc123" }
```
ğŸ“ 2. **Fetch YouTube Transcript**
```bash
GET /transcript/:videoId
Response: { transcript: "text...", segments: [{ start, end, text }] }


Uses youtube-transcript-api Python library for automatic caption retrieval.
```
ğŸ” 3. **Mood Prediction**
```bash
POST /mood/:videoId
Response:
{
  "segments": [
    { "t0": 0, "t1": 30, "mood": "calm", "confidence": 0.82 },
    { "t0": 30, "t1": 60, "mood": "hype", "confidence": 0.91 }
  ]
}
```
ğŸ§¾ 4. **Generate Summary**
```bash
POST /summary/:videoId
Response: { summary: "The video mainly has neutral tone with hype in the conclusion." }
```
---

## ğŸ–¥ï¸ Setup & Run
ğŸ”¹ Backend Setup
cd backend
pip install -r requirements.txt
uvicorn app:app --reload

ğŸ”¹ Frontend Setup
cd frontend
npm install
npm run dev


Then open the link shown in your terminal (usually http://localhost:5173/).

## ğŸ“ About the Project

This feature was developed by me as part of a Minor Project during B.Tech in Computer Science.

The goal was to research and prototype AI-driven emotion analysis for videos, focusing on enhancing user engagement and content navigation through emotional understanding.
